<!DOCTYPE HTML>
<!--
	Landed by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<!-- // The code in this client is based on the code of Robert Hawkins: https://github.com/hawkrobe/MWERT
 -->
<html>
	<head>
		<title>Reinforcement learning lab</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!-- [if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif] -->
		<link rel="stylesheet" href="../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		
		<!-- Generics -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/math.min.js"></script>
			<script src="../assets/js/jquery.csv.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/jquery.dropotron.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/skel.min.js"></script>
			<script src="../assets/js/util.js"></script>			

			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../assets/js/main.js"></script>
		<!-- Math KaTeX -->
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
			<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
		<!-- Plotting: Plot.ly JS -->
			<script type="text/javascript" src="../assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_SVG"></script>
      <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
	</head>
	<body>

	<img id="dpsquirrel" src="images/deadpool.png" style="display:none;z-index:1;position:absolute;"/>
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">

					<h1 id="logo"><a href="../index.html">Reinforcement learning lab</a></h1>
					<nav id="nav">
						<ul>
							<li><a href="../index.html">Learn</a></li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h2>Reinforcement learning lab</h2>
							<p>PSYCH 50</p>
						</header>

						<!-- Content -->
							<section id="content">

<div id="block1">
	<h2>Reinforcement learning lab</h2>
	<p>This final lab is focused on helping you understand the reinforcement learning models we use in cognitive neuroscience. We want you to both realize their usefuleness but also their inherent limitations. Learning is complex--and reinforcement learning is able to capture many forms of learning. That said, there are many caveats, some of which we will discuss today. There is now an entire field dedicated to reinforcement learning but we're going to focus narrowly on just one specific idea: that you can estimate the <b>value</b> of an action by comparing your prediction against reality over and over.</p>
	<h1>Learning goals</h1>
	After this lab you should have a better understanding of the following:
	<ul>
		<li>How dopamine neurons encode a prediction error signal</li>
		<li>How the prediction error signal can be leveraged to learn the true value of an action by integrating over time</li>
		<li>Examples of when simple reinforcement algorithms are effective and when they fail</li>
	</ul>
</div>

<div id="block2">
	<h2>Demo</h2>
	<p>The first part of this tutorial is a live demonstration of reinforcement learning in action. You will take on the role of a squirrel. Your goal is to fatten yourself up to survive the Palo Alto winter. We all know how bad it can get, so collect a lot of apples! There will be three trees, each of which drops apples. Click on a tree to visit it--after a few seconds it will drop an apple (or a leaf, if you're unlucky). Some trees are better than others, so you'll have to search among all of the trees to find the best tree... good luck!</p>
	<p>Head over to: <a href="http://gru.stanford.edu:8080" target="_blank">our local forest</a> to get started on the demo. Your TA will tell you which section to join.</p>
</div>

<div id="block3">
	<h2>Verbal model</h2>
	<p>Along with your class you should have had a discussion about a verbal model of learning in the apple hoarding task. Your model should have ended up being something like: "I keep an estimate of how many apples each tree drops in my mind. I pick a tree to go to and compare how many apples I get against what I expected. I adjust up or down accordingly". This is the verbal model that we're now going to translate into a computational one.</p>
	<p>We've done this before so we're now going to dig into more of the details than we have in past computational models. The first thing to do here is to take the computational goal we've described (the verbal model) and turn it into a mathematical equation. This is important: our goal as cognitive neuroscientists is to connect the dots between behavior and the brain. Formalizing learning will allow us to make specific predictions, both about what should be represented by the brain but also about future behaviors. These predictions are the key to testing whether reinforcement learning is how a squirrel might solve this particular task.</p>
	<p>The first part of our verbal model was: <b>keep an estimate of how many apples each tree drops</b>. For now we're going to model a world in which you make decisions about only one tree. What this means is that there's some variable that is going to keep track of our <b>knowledge</b> for a particular tree. We'll call this variable <span id="katex3-1"></span>, for apples:</p>
	<div id="katex1"></div>
	<p><span id="katex3-4"></span> should probably start at our best guess about trees (before we get any information), which you could call your prior knowledge. We'll start at zero for simplicity, which sort of implies we're pessimistic. The second part of the verbal model says, "I pick a tree and compare how many apples I get against what I expected". Let's call the number of apples you get on each visit R, for <b>reward</b>. We'll call the difference between your knowledge and the reward the <b>reward prediction error</b>:</p>
	<div id="katex2"></div>
	<p>Notice that we indexed <span id="katex3-2"></span> with a little <span id="katex3-3"></span>? That's because we're talking about a specific <b>v</b>isit. Your knowledge about how many apples a tree drops is going to change over time so we need to be specific about when exactly we're talking about.</p>
</div>

<div id="block4">
	<h2>Reward prediction error</h2>
	<p>To help you get a sense for how prediction error is useful let's imagine a world with a single tree. On each step we're going to let you make a guess of how many apples will fall and then check your answer. We won't tell you how many apples actually fell, we're just going to tell you whether the <b>prediction error</b> was positive (more apples fell than you expected) or negative. The point of this demo is to reinforce the idea that you don't need absolute knowledge to do well at estimating something's value, as long as you can integrate information over time. This should remind you of a similar story we heard about LIP neurons and MT!</p>
<!-- 	<canvas id="canvas4" height=400 width=400></canvas>
 -->	<br>
	<input id="guess4v" type="range" style="width:300px;" name="Guess" min="0" max="100" step="1" value="0" oninput="out4(this.value);"><span id="guess4out">Guess: 0</span>
	<a class="button" id="guess4" onclick="run4();">Visit tree</a><br>
	<span id="out4"></span>
	<br>
	<div id="part24">
		<br>
		<p>Nice work! During that last round you had absolute information with no noise. But in the real world the rewards you get are often noisy (on average the tree might drop an apple 50% of the time, but from moment to moment it will either drop one or not). We're now going to repeat what you just did, but with noise. </p>
	</div>
	<div id="plot4"></div>
	<div id="end4">
		<br>
		<p>Nice work! Hopefully the reward prediction error makes sense.</p>
	</div>
</div>

<div id="block5">
	<h2>Reward prediction error in the brain</h2>
	<p>In lecture we learned about how neurons that produce dopamine encode the probability of rewards and their related uncertainty. Think back to your own experience: when you visited a tree you probably made a prediction about how many apples you would get--and then felt surprise or frustration when you got more or less than that amount. This is exactly the kind of information that we think dopamine neurons encode at different times. Take a look at the setup below. We have a squirrel who is going to imagine a tree (A, B, or C), you can click in the thought bubble to change which tree the squirrel is imagining. Tree A always drops an apple, tree B drops an apple half of the time, and tree C never drops an apple. On the right there's a recording of a dopamine neuron in the squirrel's ventral tegmental area. To see what happens when the squirrel actually visits a tree you can give the squirrel an apple, or not, by clicking on the apple or leaf at the bottom. In the neural trace you'll first see activity related to the squirrel predicting it's reward, then a moment later, the squirrel will receive the apple or leaf, and you'll see the neuron's response.</p>
	<!-- STUFF -->
	<!-- 200 px of canvas is reserved for the recording output -->
	<canvas id="canvas_brain" width=1000 height=400></canvas> 
	<p>Before going on make sure you understand how this dopamine neuron circuit works:</p>
	<ul>
		<li>If the squirrel expects an apple and gets it, why doesn't the dopamine neuron fire for when the apple arrives?</li>
		<li>If the squirrel doesn't expect an apple and gets it, why then does the dopamine neuron fire?</li>
		<li>What happens in the intermediate conditions, when the squirrel believes that the tree only drops an occasional apple?</li>
	</ul>
	<p>As a reminder about how dopamine neurons work, so that you can compare against what you get above, take a look at the plot below. It covers three of the conditions you can create above. Before you continue make sure you can re-create all three traces in the image as some combination of predictions and actual rewards for the squirrel.</p>
	<img src="images/dopamine_neurons.png"/>
</div>

<div id="block6">
	<h2>Rescorla-Wagner</h2>
	<p>The last thing we need to think about is the <b>learning rate</b>. The intuition we want you to have for the learning rate is that it indexes how much you trust the reward prediction error. If there is a lot of variability in the world then you shouldn't trust it too much, you want a low learning rate to give yourself time to build up a good representation of the world. If the world is very stable and there isn't any noise then you can trust the reward prediction error precisely.</p>
	<p>Adding the learning rate, the complete model that we've now built up looks as follows:</p>
	<div id="katex3"></div>
	<p>In English: to update our estimate of a tree's value (currently <b>v</b>, the next visit's predicted value will be <b>v+1</b>) we take our current prediction, <span id="katex5-1"></span>, and add to it the reward prediction error, reduced by the learning rate which is shown as alpha.</p>
	<p>Check out the graph below. It shows you the reward prediction error on each visit and the current estimate of the tree's value. We want you to get a sense for how the learning rate changes the evolution of <span id="katex5-2"></span> over time--try tweaking the learning rate and then take a look at the examples we've put before.</p>
	<input type="range" style="width:200px;" min="0.01" max="1.00" step="0.01" value="0.50" oninput="learningrate5(this.value);"><span id="alpha5">Learning rate = 0.50</span><br>
	<div id="plot5"></div>
	<br>
	<p>To help orient you to the graph go through these activities and make sure you understand each question before continuing:</p>
	<ul>
		<li>Try setting the learning rate to a very small value. What verbal model would this translate to? What would your decisions at each time point rely on?</li>
		<li>What if the learning rate is one? How then do your decisions change over time?</li>
		<li>Why does the RPE decrease, whereas the <span id="katex5-3"></span> value increases and then stabilizes?</li>
		<li>What value does the learning rate need to be set to so that in the time span available the <span id="katex5-4"></span> value stabilizes and the RPE is minimized. Can the RPE ever go to zero? Why/why not?</li>
	</ul>
</div>

<div id="block7">
	<h2>Exploit? Or explore?</h2>
	<p>So far your life as a squirrel has been easy. You were all alone in the forest visiting trees by yourself and nobody bothered you. But what would happen if other squirrels were visiting the trees along with you, is it still always optimal to head for the tree that drops the most apples? If everybody were to choose that same strategy, which we call <b>exploiting</b>, then someone would lose out. This is one of a few situations in which it can be advantageous to continue <b>exploring</b> the other options. Another situation in which this often comes up is when the value of a reward changes over time. Recall the Wisconsin card sort task--if the rules suddenly change and you stop being rewarded for the rule you are holding in mind it becomes advantageous to search for a new rule.</p>
	<h2>Competition</h2>
	<p>We're going to repeat the demonstration now, but with two changes. First, you're now going to be able to see the other students on the TA's screen. Second, everybody is now competing. The trees still only drop apples on some visits (some trees still drop apples more frequently), but now the stock of apples is limited. Even when a tree drops apples you might still get nothing. Should you persist at the tree you think is best? Or should you explore other options when this happens. We'll see who can come up with the best strategy! Connect to <a href="http://gru.stanford.edu:8080" target="_blank">our local forest</a> again and follow your TAs instructions. You'll have to wait for other students to catch up, so this is a good time to ask questions.</p>
</div>
<!-- <div id="block6">
	<h2>Multiple trees</h2>
	<p>In the demo you kept in mind the values of three trees. We can do this just fine with Rescorla-Wagner. Each tree simply gets updated when it gets visited, and otherwise the value doesn't change. But there's a different problem with having multiple problems which goes beyond reinforcement learning. Reinforcement learning is about discovering the <b>value</b> of taking different actions in the world. But given different choices with known values, how should you pick between them? In other words you were doing two things during the task: determining values and then using those values to make decisions. This next section will be focused on <b>decision making</b>.</p>
	<p>For today we want to contrast two ways of picking which tree to visit. Imagine that you believe that trees A, B, and C drop 1, 2, and 3 apples on average. Or as variables:</p>
	<div id="katex6"></div>
	<p>One rule you could use to decide where to go next is winner-take-all. Since tree C is worth the most you would go there. There are two possible outcomes for the <i>following</i> visit. If on this visit your belief about tree C improves or stays the same, you'll keep repeating this action. But if you were to get far less apples than expected, you might reduce your estimate of tree C's value enough to make tree B the best choice. In a winner-take-all rule the only way to change choices is for your current choice to get a lot worse. Formally we can represent winner take all by saying that the probability of visiting a tree is a binary value (0 or 1), based on whether it is the maximum:</p>
	<!-- <div id="katex6-2"></div> -->
	<!-- <p>Alternatively you could be choosing probabilistically. That is, how frequently you choose an option will depend on how you value it compared to other options. This rule is called <b>softmax</b>. Softmax sets the probability of going to a tree to be that trees value, divided by the total value available among all trees:</p>
	<div id="katex6-3"></div>
	<p>The advantage of softmax over winner-take-all is that it allows you to continue <b>exploring</b> rather than <b>exploiting</b> the same tree over and over. If it turns out your estimates aren't perfect then exploring is often a better strategy than exploiting. Let's run a simulation to help you understand this idea. Let's call softmax an "explore" strategy and winner-take-all the "exploit" strategy. We're going to have three squirrels below, a red, purple, and blue squirrel. The red and blue squirrels will represent the exploit and explore strategies, respectively (in reality they spend 90% of their time in their preferred mode). The purple squirrel spends half of its time exploiting, and half of its time exploring. Which squirrel do you expect will gain the most apples?</p>
	<p>Whatever your guess is we hope you realized that <i>it depends</i>! If the world is really noisy then you should explore more, if the world is very stable, then you should exploit. You can use the slider bar to control how noisy the world is, the noise is calculated for each visit by randomly adding or subtracting apples to the true value of each tree. The number of apples maxes out at the noise value. Try it out and then make sure you understand the questions at the bottom. Click anywhere in the forest to reset the simulation.</p>
	<input type="range" style="width:200px;" min="1" max="5" step="1" value="1" oninput="noise6(this.value);"><span id="noise6">Noise = 1</span><br>
	<canvas id="canvas6" height=500 width=800 onclick="reset6();"></canvas>
	<br><span id="win6">Number of wins. red/exploit: 0 blue/explore: 0 purple/mix: 0</span>
	<div id="plot6"></div>
	<a class="button" onclick="fast();">Speed things up!</a>
	<br>
</div> -->
<!-- 
<div id="block8">
</div> -->

<div id="endblock">
	<h2>Tutorial complete!</h2>
	<p> Your TAs would be happy to discuss any aspect of the tutorial. As a reminder, you should now have a better understanding of the following:</p>
	<ul>
		<li>How dopamine neurons encode a prediction error signal</li>
		<li>How the prediction error signal can be leveraged to learn the true value of an action by integrating over time</li>
		<li>Examples of when simple reinforcement algorithms are effective and when they fail</li>
	</ul>
</div>


							</section>
							<section id="continue">
								<a class="button special" onclick="prev();">Go back</a>
								<a class="button special" onclick="next();">Continue</a>
							</section>
						<!-- End Content -->
					</div>
				</div>

			<!-- Footer -->
				<footer id="footer">
					<ul class="copyright">
						<li>&copy; Dan Birman 2015-Present</li><li><a href="https://github.com/dbirman/web/" target="_blank">Code<a/></li>
					</ul>
				</footer>

		</div>

		<!-- Local CSS -->
			<!-- <link rel="stylesheet" href="psych50.css" /> -->
		<!-- Scripts -->
			<script src="../assets/js/shared_code.js"></script>
			<script src="psych50_rl.js"></script>

			<script>launch();</script>
	</body>
</html>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-34324563-2', 'auto');
  ga('send', 'pageview');

</script>
